{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8hPeIx5Y4iJYM6fM0sAEp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ritika-kalyanshetti/Hands-on-ML/blob/main/HML_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3jqq-urnpZ0",
        "outputId": "bb192c8d-162e-4fde-bd2f-ab634b339ebf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unit-Porgram-12 ...THIS IS  THE FIRST PROGRAM TO SPLIT SENTENCES\n",
            "This is first type\n",
            "\n",
            "\n",
            "\n",
            "['How', 'to', 'tokenize?', 'Like', '$$', 'a', 'boss.']\n",
            "['Google', 'is_', 'accessible', 'via', 'http://www.google.com']\n",
            "['1000', 'new', 'followers!', 'a', '#TwitterFamous']\n"
          ]
        }
      ],
      "source": [
        "print('Unit-Porgram-12 ...THIS IS  THE FIRST PROGRAM TO SPLIT SENTENCES')\n",
        "lines=[\n",
        "    'How to tokenize?\\nLike $$\\t a boss.',\n",
        "    'Google is_ accessible via http://www.google.com',\n",
        "    '1000 new followers! a #TwitterFamous'\n",
        "    ]\n",
        "\n",
        "print('This is first type\\n\\n\\n')\n",
        "for line in lines:\n",
        "    print(line.split()) # It will split 'How to tokenize?\\nLike $$\\t a boss.' to 'How', 'to', 'tokenize?', 'Like', '$$', 'a', 'boss.'\n",
        "                        # It removes $$ and #"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lines2=[\n",
        "    'India has many historical\\n\\n\\n\\n\\n\\n monuments\\t',\n",
        "    'Many information\\t\\t\\t\\t\\t #can be %obtained from http://www.google.com',\n",
        "    '1000 new followers! a #TwitterFamous'\n",
        "    ]\n",
        "\n",
        "print('This is first type\\n\\n\\n')\n",
        "for line in lines2:\n",
        "    print(line.split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70-_uy2znr0o",
        "outputId": "99aa034d-d3e9-4edc-8812-e45ffa7a1955"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is first type\n",
            "\n",
            "\n",
            "\n",
            "['India', 'has', 'many', 'historical', 'monuments']\n",
            "['Many', 'information', '#can', 'be', '%obtained', 'from', 'http://www.google.com']\n",
            "['1000', 'new', 'followers!', 'a', '#TwitterFamous']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  print('2.\tSecond Program for splitting sentences')\n",
        "print('THIS IS THE SECOND PROGRAM FOR SPLITTING SENTENCES')\n",
        "lines=[\n",
        "    'How to tokenize?\\nLike a boss.',\n",
        "    'Google is_ accessible via http://www.google.com',\n",
        "    '1000 new %followers! a #TwitterFamous'\n",
        "    ]\n",
        "\n",
        "#A Regular Expressions (RegEx) is a special sequence of characters\n",
        "#that uses a search pattern to find a string or set of strings.\n",
        "#It can detect the presence or absence of a text by matching\n",
        "#it with a particular pattern, and also can split a pattern into\n",
        "#one or more sub-patterns. Python provides a re module that supports\n",
        "#the use of regex in Python. Its primary function is to offer a search,\n",
        "#where it takes a regular expression and a string.\n",
        "\n",
        "# import re\n",
        "\n",
        "import re\n",
        "#_token_pattern=r\"\\w+\"\n",
        "_token_pattern=r\"\\w\"\n",
        "\n",
        "\n",
        "#Python’s re.compile() method is used to compile a regular expression pattern provided\n",
        "#as a string into a regex pattern object (re.Pattern).\n",
        "#Later we can use this pattern object to search\n",
        "#for a match inside different target strings using regex methods\n",
        "\n",
        "token_pattern=re.compile(_token_pattern)\n",
        "\n",
        "\n",
        "print('This is second type\\n\\n\\n')\n",
        "for line in lines:\n",
        "    print(token_pattern.findall(line))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5Bhg-8xpPLO",
        "outputId": "1bccd2ca-1093-4b64-f8dd-1de901c2ffce"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.\tSecond Program for splitting sentences\n",
            "THIS IS THE SECOND PROGRAM FOR SPLITTING SENTENCES\n",
            "This is second type\n",
            "\n",
            "\n",
            "\n",
            "['H', 'o', 'w', 't', 'o', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'e', 'L', 'i', 'k', 'e', 'a', 'b', 'o', 's', 's']\n",
            "['G', 'o', 'o', 'g', 'l', 'e', 'i', 's', '_', 'a', 'c', 'c', 'e', 's', 's', 'i', 'b', 'l', 'e', 'v', 'i', 'a', 'h', 't', 't', 'p', 'w', 'w', 'w', 'g', 'o', 'o', 'g', 'l', 'e', 'c', 'o', 'm']\n",
            "['1', '0', '0', '0', 'n', 'e', 'w', 'f', 'o', 'l', 'l', 'o', 'w', 'e', 'r', 's', 'a', 'T', 'w', 'i', 't', 't', 'e', 'r', 'F', 'a', 'm', 'o', 'u', 's']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('2.\tSecond Program for splitting sentences')\n",
        "print('THIS IS THE SECOND PROGRAM FOR SPLITTING SENTENCES')\n",
        "lines=[\n",
        "    'India is great'\n",
        "    ]\n",
        "\n",
        "#A Regular Expressions (RegEx) is a special sequence of characters\n",
        "#that uses a search pattern to find a string or set of strings.\n",
        "#It can detect the presence or absence of a text by matching\n",
        "#it with a particular pattern, and also can split a pattern into\n",
        "#one or more sub-patterns. Python provides a re module that supports\n",
        "#the use of regex in Python. Its primary function is to offer a search,\n",
        "#where it takes a regular expression and a string.\n",
        "\n",
        "# import re\n",
        "\n",
        "import re\n",
        "#_token_pattern=r\"\\w+\"\n",
        "\n",
        "_token_pattern=r\"\\w\"\n",
        "\n",
        "\n",
        "#Python’s re.compile() method is used to compile a regular expression pattern provided\n",
        "#as a string into a regex pattern object (re.Pattern).\n",
        "#Later we can use this pattern object to search\n",
        "#for a match inside different target strings using regex methods\n",
        "\n",
        "token_pattern=re.compile(_token_pattern)\n",
        "\n",
        "print('The given sentence',lines)\n",
        "print('This is second type\\n\\n\\n')\n",
        "for line in lines:\n",
        "    print(token_pattern.findall(line))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sGYDoP7qApt",
        "outputId": "53c73e00-e672-4594-bca6-4faada7fcec8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.\tSecond Program for splitting sentences\n",
            "THIS IS THE SECOND PROGRAM FOR SPLITTING SENTENCES\n",
            "The given sentence ['India is great']\n",
            "This is second type\n",
            "\n",
            "\n",
            "\n",
            "['I', 'n', 'd', 'i', 'a', 'i', 's', 'g', 'r', 'e', 'a', 't']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('2.\tSecond Program for splitting sentences')\n",
        "print('THIS IS THE SECOND PROGRAM FOR SPLITTING SENTENCES')\n",
        "lines=[\n",
        "    'How to tokenize?\\nLike a boss.',\n",
        "    'Google is_ accessible via http://www.google.com',\n",
        "    '1000 new %followers! a #TwitterFamous'\n",
        "    ]\n",
        "\n",
        "#A Regular Expressions (RegEx) is a special sequence of characters\n",
        "#that uses a search pattern to find a string or set of strings.\n",
        "#It can detect the presence or absence of a text by matching\n",
        "#it with a particular pattern, and also can split a pattern into\n",
        "#one or more sub-patterns. Python provides a re module that supports\n",
        "#the use of regex in Python. Its primary function is to offer a search,\n",
        "#where it takes a regular expression and a string.\n",
        "\n",
        "# import re\n",
        "\n",
        "import re\n",
        "#_token_pattern=r\"\\w+\"\n",
        "#_token_pattern=r\"\\w\"\n",
        "_token_pattern=r\"\\w+\"\n",
        "\n",
        "token_pattern=re.compile(_token_pattern)\n",
        "print('This is second type\\n\\n\\n')\n",
        "for line in lines:\n",
        "    print(token_pattern.findall(line))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpscOrfSs2i1",
        "outputId": "e36fd757-3081-41bf-9348-1297fb5eaeed"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.\tSecond Program for splitting sentences\n",
            "THIS IS THE SECOND PROGRAM FOR SPLITTING SENTENCES\n",
            "This is second type\n",
            "\n",
            "\n",
            "\n",
            "['How', 'to', 'tokenize', 'Like', 'a', 'boss']\n",
            "['Google', 'is_', 'accessible', 'via', 'http', 'www', 'google', 'com']\n",
            "['1000', 'new', 'followers', 'a', 'TwitterFamous']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('\tProgram to split sentences where one character words are not considered')\n",
        "print('THIS  TYPE-3')\n",
        "lines=[\n",
        "    'India is a #great country'\n",
        "    ]\n",
        "\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "#_token_pattern=r\"(?u)\\b\\w\\w+\\b\"\n",
        "#_token_pattern=r\"\\b\\w\\w+\\b\"\n",
        "#_token_pattern=r\"\\w\\w+\\b\"\n",
        "_token_pattern=r\"\\w\\w+\"\n",
        "\n",
        "token_pattern=re.compile(_token_pattern)\n",
        "print('The given sentences ',lines)\n",
        "print('This is third type\\n\\n')\n",
        "for line in lines:\n",
        "    print(token_pattern.findall(line))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nljzYhKHvBDM",
        "outputId": "26975abb-31ac-4664-a4a3-cd9769f68d60"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tProgram to split sentences where one character words are not considered\n",
            "THIS  TYPE-3\n",
            "The given sentences  ['India is a #great country']\n",
            "This is third type\n",
            "\n",
            "\n",
            "['India', 'is', 'great', 'country']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('a. Program to split the words')\n",
        "print('\tProgram to split sentences where hashtag and url are represented')\n",
        "\n",
        "lines=[\n",
        "    'How to tokenize?\\nLike a boss.',\n",
        "    'Google is_ accessible via http://www.google.com',\n",
        "    '1000 new followers! a #TwitterFamous'\n",
        "    ]\n",
        "\n",
        "#print('This is first type\\n\\n\\n')\n",
        "#for line in lines:\n",
        "#    print(line.split())\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "_token_pattern=r\"\\w+\"\n",
        "token_pattern=re.compile(_token_pattern)\n",
        "\n",
        "def tokenizer(line):\n",
        "    line=line.lower()\n",
        "    return token_pattern.findall(line)\n",
        "\n",
        "\n",
        "print('This is fourth type\\n\\n\\n')\n",
        "\n",
        "for line in lines:\n",
        "    print(tokenizer(line))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGmWzGIfzDxC",
        "outputId": "b03a49ba-85b0-4623-bf66-2efbdcd4b29c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a. Program to split the words\n",
            "\tProgram to split sentences where hashtag and url are represented\n",
            "This is fourth type\n",
            "\n",
            "\n",
            "\n",
            "['how', 'to', 'tokenize', 'like', 'a', 'boss']\n",
            "['google', 'is_', 'accessible', 'via', 'http', 'www', 'google', 'com']\n",
            "['1000', 'new', 'followers', 'a', 'twitterfamous']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('b. Program to split the words')\n",
        "lines=[\n",
        "    'How to tokenize?\\nLike a boss.',\n",
        "     'Google is_ accessible via http://www.google.com',\n",
        "    'Google is_ accessible via http://www.google.com.com.com.com',\n",
        "    'Google is_ accessible via https://www.google.com.com.com.com',\n",
        "    '1000 new followers! a #TwitterFamous'\n",
        "    ]\n",
        "\n",
        "#print('This is first type\\n\\n\\n')\n",
        "#for line in lines:\n",
        "#    print(line.split())\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "_token_pattern=r\"\\w+\"\n",
        "token_pattern=re.compile(_token_pattern)\n",
        "\n",
        "def tokenizer(line):\n",
        "    line=line.lower()\n",
        "\n",
        "    line=re.sub(r'http[s]?://[\\w\\.\\?]+','url',line)\n",
        "    return token_pattern.findall(line)\n",
        "\n",
        "print('This is fourth type\\n\\n\\n')\n",
        "print('\\n It converts to lower case and removes Special characters and https[s]')\n",
        "\n",
        "for line in lines:\n",
        "    print(tokenizer(line))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQ9bIg5F0huz",
        "outputId": "df92e7a0-1bc0-47b2-d087-a4302796e1ec"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b. Program to split the words\n",
            "This is fourth type\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " It converts to lower case and removes Special characters and https[s]\n",
            "['how', 'to', 'tokenize', 'like', 'a', 'boss']\n",
            "['google', 'is_', 'accessible', 'via', 'url']\n",
            "['google', 'is_', 'accessible', 'via', 'url']\n",
            "['google', 'is_', 'accessible', 'via', 'url']\n",
            "['1000', 'new', 'followers', 'a', 'twitterfamous']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('\tProgram to split sentences where hashtag and url are represented')\n",
        "print('THIS IS THE FOURTH TYPE')\n",
        "lines=[\n",
        "    'How to tokenize?\\nLike a boss.',\n",
        "    'Google is_ accessible via http://www.google.com',\n",
        "    '1000 new followers! a #TwitterFamous'\n",
        "    ]\n",
        "\n",
        "print('The given data\\n\\n\\n',lines)\n",
        "#for line in lines:\n",
        "#    print(line.split())\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "_token_pattern=r\"\\w+\"\n",
        "token_pattern=re.compile(_token_pattern)\n",
        "\n",
        "def tokenizer(line):\n",
        "    line=line.lower()\n",
        "    line=re.sub(r'http[s]?://[\\w\\/\\-\\.\\?]+','url',line)\n",
        "    line=re.sub(r'#\\w+','hashtag',line)\n",
        "    line=re.sub(r'\\d+','num',line)\n",
        "    return token_pattern.findall(line)\n",
        "\n",
        "print('This is fourth type\\n\\n\\n')\n",
        "\n",
        "for line in lines:\n",
        "    print(tokenizer(line))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PE2kH_P2-sF",
        "outputId": "fd6dc0a9-ea49-451a-bd77-ddd5eb544d6e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tProgram to split sentences where hashtag and url are represented\n",
            "THIS IS THE FOURTH TYPE\n",
            "The given data\n",
            "\n",
            "\n",
            " ['How to tokenize?\\nLike a boss.', 'Google is_ accessible via http://www.google.com', '1000 new followers! a #TwitterFamous']\n",
            "This is fourth type\n",
            "\n",
            "\n",
            "\n",
            "['how', 'to', 'tokenize', 'like', 'a', 'boss']\n",
            "['google', 'is_', 'accessible', 'via', 'url']\n",
            "['num', 'new', 'followers', 'a', 'hashtag']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('\tProgram to split sentences where hashtag and url are represented')\n",
        "print('THIS IS THE FOURTH TYPE')\n",
        "lines=[\n",
        "    'How to tokenize?\\nLike a boss.',\n",
        "    'Google is_ $accessible @via http://www.google.com',\n",
        "    '1000 10new #followers! a #TwitterFamous'\n",
        "    ]\n",
        "\n",
        "print('The given data\\n\\n\\n',lines)\n",
        "#for line in lines:\n",
        "#    print(line.split())\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "_token_pattern=r\"\\w+\"\n",
        "token_pattern=re.compile(_token_pattern)\n",
        "\n",
        "def tokenizer(line):\n",
        "    line=line.lower()\n",
        "    #line=re.sub(r'http[s]?://[\\w\\/\\-\\.\\?]+','url',line)\n",
        "    line=re.sub(r'http[s]?://[\\w\\.\\?]+','url',line)\n",
        "    line=re.sub(r'#\\w+','hashtag',line)\n",
        "    line=re.sub(r'\\d+','num',line)\n",
        "    return token_pattern.findall(line)\n",
        "\n",
        "print('This is fourth type\\n\\n\\n')\n",
        "\n",
        "for line in lines:\n",
        "    print(tokenizer(line))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkLY9HqU7BS_",
        "outputId": "107392dc-7d30-4a47-d60b-af206fa9c4d5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tProgram to split sentences where hashtag and url are represented\n",
            "THIS IS THE FOURTH TYPE\n",
            "The given data\n",
            "\n",
            "\n",
            " ['How to tokenize?\\nLike a boss.', 'Google is_ $accessible @via http://www.google.com', '1000 10new #followers! a #TwitterFamous']\n",
            "This is fourth type\n",
            "\n",
            "\n",
            "\n",
            "['how', 'to', 'tokenize', 'like', 'a', 'boss']\n",
            "['google', 'is_', 'accessible', 'via', 'url']\n",
            "['num', 'numnew', 'hashtag', 'a', 'hashtag']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "print('\tProgram on CountVectorizer to tokenize the given sentences')\n",
        "\n",
        "print('THIS IS THE PROGRAM  ON CountVectorizer')\n",
        "print('ONLY Count Vectorizer')\n",
        "\n",
        "lines=[\n",
        "    'How to tokenize?\\nLike a boss.',\n",
        "    'Google google is_ accessible via http://www.google.com',\n",
        "   '1000 new followers! a #TwitterFamous'\n",
        "   ]\n",
        "\n",
        "\n",
        "def tokenizer(line):\n",
        "    line=line.lower()\n",
        "    line=re.sub(r'http[s]?://[\\w\\?]+','url',line)\n",
        "    line=re.sub(r'#\\w+','hashtag',line)\n",
        "    line=re.sub(r'\\d+','num',line)\n",
        "    return token_pattern.findall(line)\n",
        "\n",
        "\n",
        "import re\n",
        "_token_pattern=r\"\\w+\"\n",
        "token_pattern=re.compile(_token_pattern)\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vec=CountVectorizer(lowercase=True,tokenizer=tokenizer)\n",
        "x=vec.fit_transform(lines)\n",
        "\n",
        "\n",
        "\n",
        "flight_delayed_lines=[\n",
        "    'Flight was delayed , I am not happy',\n",
        "    'Flight was not delayed,I am happy'\n",
        "    ]\n",
        "\n",
        "\n",
        "x=vec.fit_transform(flight_delayed_lines)\n",
        "\n",
        "\n",
        "xyz=pd.DataFrame(\n",
        "    x.todense(),\n",
        "    columns=vec.get_feature_names_out()\n",
        "    )\n",
        "print(xyz)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpJgbxWj8EhK",
        "outputId": "acc80453-68ef-479e-df2c-bd1ac0bc2d8d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tProgram on CountVectorizer to tokenize the given sentences\n",
            "THIS IS THE PROGRAM  ON CountVectorizer\n",
            "ONLY Count Vectorizer\n",
            "   am  delayed  flight  happy  i  not  was\n",
            "0   1        1       1      1  1    1    1\n",
            "1   1        1       1      1  1    1    1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7hxAq2q8CXoy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}